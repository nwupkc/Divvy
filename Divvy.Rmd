---
title: "Divvy"
author: "Sungwan Kim"
date: "9/5/2017"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This is a prediction of bicycle sharing usage in the city of Chicago. I have defined the usage as the number of trips taken per day. I will use historical trips and weather datasets from 2014 to 2016 to predict the usage for the first half of 2017. I have decided to exclude trips data from 2013 as the number of rides during first few months are noticeably low as Divvy just started out its service in the Windy City.

### Load Data

```{r packages, message = FALSE}
# Load packages
library('tidyverse')
library('lubridate')
library('leaps')
library('tree')
library('randomForest')
library('gbm')
```

To jump right into the Exploratory Data Analysis and Feature Engineering, I have written detailed data preparation at the end.

```{r import, message = FALSE}
train <- read_csv("train.csv")
test <- read_csv("test.csv")
```
```{r check}
# check data
str(train)
```

There are few missing values in our training data set. These are from the days when there were no trips, so we input the adequate values accordingly.

```{r NAs}
which(is.na(train$Trips))
train[which(is.na(train$Trips)),]$Trips <- 0
```


## Feature Engineering

Let's shed more light into what is driving the Divvy usage. First, I will define usage to be the number of trips taken per day.

```{r graph1, fig.show = "hold"}
ggplot(train, aes(Date, Trips)) + geom_line()
```

The graph shows strong seasonal pattern. I will create a variable called quarter to capture this pattern when predicting.

```{r quarter}
train$Quarter <- 1
train$Quarter[month(train$Date) %in% 4:6] <- 2
train$Quarter[month(train$Date) %in% 7:9] <- 3
train$Quarter[month(train$Date) %in% 10:12] <- 4
train$Quarter <- as.factor(train$Quarter)
```

I will create day of the week variable to see the shorter usage pattern.

```{r graph2, fig.show = "hold"}
train <- train %>% 
  mutate(Wday = wday(Date, label = TRUE))
ggplot(train, aes(Wday, Trips)) + geom_boxplot()
```

It seems like usage during week days are little higher than the usage during weekends.

```{r graph3, fig.show = "hold"}
train %>% 
  ggplot(aes(Wday, Trips)) +
    geom_boxplot() +
    facet_grid(.~Quarter)
```

It seems like the usage for day of the week vary for different quarters.

We add the same additional features to our test set.

```{r addvartest}
test <- test %>% 
  mutate(Wday = wday(Date, label = TRUE))
test$Quarter <- 1
test$Quarter[month(test$Date) %in% 4:6] <- 2
test$Quarter[month(test$Date) %in% 7:9] <- 3
test$Quarter[month(test$Date) %in% 10:12] <- 4
test$Quarter <- as.factor(test$Quarter)
levels(test$Quarter) <- levels(train$Quarter)
```

# Prediction

### Linear Regression

I will first use linear regression as a starting point. While not as fancy as newer models, linear regression has had time to build statistical rigor which newer models do not have.

```{r lm1}
lm.fit <- lm(Trips ~ Temperature_Avg + Dew_Point_Avg + Humidity_Avg + Sea_Level_Press_Avg + Visibility_Avg + Wind_Avg + Precipitation + Events + Wday + Quarter, train)
summary(lm.fit)
```

R-squared represents a goodness of fir and it is 0.6451 which means large portion of the variance is explained by our model. The p-value associated with the F-statistics suggest that we can reject the null hypothesis. This means at least one of the regressors are is associated with the increase in usage. The model has found temperature, sea level pressure, wind, precipitation, rain, thunderstorm, and quarter2 to be statistically significant based on the p-values associated with t-statistics. 

So, how did our model do in terms of prediction? I decided to use **root mean squared error**(RMSE) which measures how far the predicted usage differs from the actual usage. RMSE is a popular metric for assessing model accuracy.

```{r rmse}
# RMSE
(RMSE <- sqrt(mean((test$Trips - predict(lm.fit, test))^2)))
```

The linear model has a root mean squared error(RMSE) of `r round(RMSE, digits=3)`.

We might want to have an interaction term between day of the week and quarter variables as we have confirmed above that the usage differ by weekdays for different quarters.

```{r lm2}
lm.fit2 <- lm(Trips ~ Temperature_Avg + Dew_Point_Avg + Humidity_Avg + Sea_Level_Press_Avg + Visibility_Avg + Wind_Avg + Precipitation + Events + Wday * Quarter, train)
summary(lm.fit2)
#RMSE
(RMSE <- sqrt(mean((test$Trips - predict(lm.fit2, test))^2)))
```

Our second linear model has a RMSE of `r round(RMSE, digits=3)`.

One possible problem of the linear regression is non-constant variances of errors, or heteroscedasticity. Heteroscedasticity occurs when assumption homoskedasticity (i.e. Var(Ui|Xi) = Var(Ui)) is violated. We can detect heteroskedasticity by identifying a funnel shape in the residuals plot.

```{r graph4, fig.show = "hold"}
par(mfrow=c(2,2))
plot(lm.fit2)
```

We can solve heteroskedasticity by log-transformation. Since we have two trips value with 0, we cannot take log straight away since log(0)=−∞ . One way to bypass this problem is to add 1 (i.e. log(x+1)).

```{r lm3, fig.show = "hold"}
lm.fit3 <- lm(log(Trips+1) ~ Temperature_Avg + Dew_Point_Avg + Humidity_Avg + Sea_Level_Press_Avg + Visibility_Avg + Wind_Avg + Precipitation + Events + Wday * Quarter, train)
par(mfrow=c(2,2))
plot(lm.fit3)

# RMSE
(RMSE <- sqrt(mean((test$Trips - exp(predict(lm.fit3, test))-1)^2)))
```

We treated heteroskedasticity, but the prediction result suffered as RMSE score skyrocketed to `r round(RMSE, digits=3)`. Another way to treat heteroskedasticy would be to use square root transformation.

```{r lm4, fig.show = "hold"}
lm.fit4 <- lm(sqrt(Trips) ~ Temperature_Avg + Dew_Point_Avg + Humidity_Avg + Sea_Level_Press_Avg + Visibility_Avg + Wind_Avg + Precipitation + Events + Wday * Quarter, train)
par(mfrow=c(2,2))
plot(lm.fit4)

# RMSE
sqrt(mean((test$Trips - predict(lm.fit4, test)^2)^2))
```

We have done better than our orignial linear model! We have found a model which is both more accurate and robust.

### Feature Selection

R-squared cannot be used to select among a set of different variables as it will always increase with an additional variable. To choose the optimal model with the right numbers of varaibles we will consdier Adjusted R-squared, Bayesian information criterion(BIC), and Mallow’s Cp all of which adds penalty that increases with more variables.

```{r feature_selection, fig.show = "hold"}
regfit.full <- regsubsets(sqrt(Trips) ~ Temperature_Avg + Dew_Point_Avg + Humidity_Avg + Sea_Level_Press_Avg + Visibility_Avg + Wind_Avg + Precipitation + Events + Wday * Quarter, train)
reg.summary <- summary(regfit.full)
# R-squared statistic always increase with additional variable
reg.summary$rsq
# plotting RSS, adjusted R-squared, Cp, and BIC
par(mfrow=c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
which.max(reg.summary$adjr2)
points(8, reg.summary$adjr2[8], col = "red", cex = 2, pch = 20)
plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
which.min(reg.summary$cp)
points(8, reg.summary$cp[8], col = "red", cex = 2, pch = 20)
plot(reg.summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
which.min(reg.summary$bic)
points(8, reg.summary$bic[8], col = "red", cex = 2, pch = 20)
```

All three crieria tell us that the best number of feature is 8.

### Decision Trees

Next, we consider the decision trees model to predict the usage. Decision tree method involve segmenting predictor region into multiple regions and using the region the observation falls into to make a prediction. Among many advantages of decision trees is its high interpretability.

```{r decision_trees, fig.show = "hold"}
# set random seed
set.seed(11)
tree.divvy <- tree(Trips ~ Temperature_Avg + Dew_Point_Avg + Humidity_Avg + Sea_Level_Press_Avg + Visibility_Avg + Wind_Avg + Precipitation + Events + Wday + Quarter, train)
summary(tree.divvy)
# plot tree
plot(tree.divvy)
text(tree.divvy, pretty = 0)
# RMSE
sqrt(mean((predict(tree.divvy, newdata = test) - test$Trips)^2))
```

We did better than linear models. Decision tree is not known for prediction accuracy, but it serves as a building block to create more powerful machine learning algorithms.

### Ensemble Models

In this section we will consider three ensembles models: Bagging, Random Forests, and Boosting. All these methods combine a large number of decision trees to yield prediction with high accuracy at the cost of loss in interpretation.

We will first convert character vectors to factors so we can use randomForest package.

```{r factors}
sapply(train, typeof)
sapply(test, typeof)
train$Events <- as.factor(train$Events)
test$Events <- as.factor(test$Events)
```

Instead of using decision trees which suffers from high variance, we can use bootstrap aggregation or bagging to reduce the variance of a statistical learning method. Bagging takes repeated samples from our training set, grows hundreds of trees and averages all the predictions.

```{r bagging}
set.seed(11)
bag.divvy <- randomForest(Trips ~ Temperature_Avg + Dew_Point_Avg + Humidity_Avg + Sea_Level_Press_Avg + Visibility_Avg + Wind_Avg + Precipitation + Events + Wday + Quarter, train, mtry = 9, importance = TRUE)

#RMSE
sqrt(mean((predict(bag.divvy, newdata = test) - test$Trips)^2))
```

Bagging result shows improved accuracy over a single tree. Bagging model is a special case of random forests when we use all the features available.

Random forests works similarly as bagging but instead decorrelates the individual trees by taking a random sample of predictors when building each tree. The default number of features used for each tree is square root of the all available features. In this case it is 3.

```{r random_forests}
rf.divvy <- randomForest(Trips ~ Temperature_Avg + Dew_Point_Avg + Humidity_Avg + Sea_Level_Press_Avg + Visibility_Avg + Wind_Avg + Precipitation + Events + Wday + Quarter, train, mtry = 3, importance = TRUE)

# RMSE
sqrt(mean((predict(rf.divvy, newdata = test) - test$Trips)^2))
```

Let’s look at relative variable importance by plotting the mean decrease in Gini calculated across all trees. Temperature is by far the most important variable.

```{r varimp, fig.show = "hold"}
importance(rf.divvy)
varImpPlot(rf.divvy)
```

Boosting works similarly as bagging, but the trees are grown on top the previous trees using information they have learned. Boosting does not involve sampling as each tree is fit on a modified version of the original data set.

```{r boosting, fig.show = "hold"}
set.seed(11)
boost.divvy <- gbm(Trips ~ Temperature_Avg + Dew_Point_Avg + Humidity_Avg + Sea_Level_Press_Avg + Visibility_Avg + Wind_Avg + Precipitation + Events + Wday + Quarter, train, distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
# summary() function produces a relative influence plot and relative influence statistics
summary(boost.divvy)

# RMSE
sqrt(mean((predict(boost.divvy, newdata = test,n.trees = 5000) - test$Trips)^2))
```

# Conclusion

We have used linear regression, decision trees, bagging, random forests, and boosting models to predict the Divvy usage.Ensemble models performed better than simple models like linear regression and decision trees. The RMSE associated with bagging is 2025.091 compared to a simple linear regression's 2476.261 which is 18% improvement in accuracy.

On a side note, if our goal was interpretability of the model, then linear regression and decision tree is far more interpretable than complex ensemble models. However, reducing RMSE was of utmost importance as our primary goal was prediction accuracy. 


## Data Preparation (Optional)

### Training Data Set

```{r train, eval = FALSE}
trips_2014_Q1Q2 <- read_csv("Divvy_Trips_2014_Q1Q2.csv")
trips_2014_Q3_07 <- read_csv("Divvy_Trips_2014-Q3-07.csv")
trips_2014_Q3_0809 <- read_csv("Divvy_Trips_2014-Q3-0809.csv")
trips_2014_Q4 <- read_csv("Divvy_Trips_2014-Q4.csv")
trips_2015_Q1 <- read_csv("Divvy_Trips_2015-Q1.csv")
trips_2015_Q2 <- read_csv("Divvy_Trips_2015-Q2.csv")
trips_2015_Q3_07 <- read_csv("Divvy_Trips_2015_07.csv")
trips_2015_Q3_08 <- read_csv("Divvy_Trips_2015_08.csv")
trips_2015_Q3_09 <- read_csv("Divvy_Trips_2015_09.csv")
trips_2015_Q4 <- read_csv("Divvy_Trips_2015_Q4.csv")
trips_2016_Q1 <- read_csv("Divvy_Trips_2016_Q1.csv")
trips_2016_Q2_04 <- read_csv("Divvy_Trips_2016_04.csv")
trips_2016_Q2_05 <- read_csv("Divvy_Trips_2016_05.csv")
trips_2016_Q2_06 <- read_csv("Divvy_Trips_2016_06.csv")
trips_2016_Q3 <- read_csv("Divvy_Trips_2016_Q3.csv")
trips_2016_Q4 <- read_csv("Divvy_Trips_2016_Q4.csv")
trips_2017_Q1 <- read_csv("Divvy_Trips_2017_Q1.csv")
trips_2017_Q2 <- read_csv("Divvy_Trips_2017_Q2.csv")

trips <- rbind(trips_2014_Q1Q2, trips_2014_Q3_07, trips_2014_Q3_0809, trips_2014_Q4, 
               trips_2015_Q1, trips_2015_Q2, trips_2015_Q3_07, trips_2015_Q3_08,
               trips_2015_Q3_09, trips_2015_Q4, trips_2016_Q1, trips_2016_Q2_04,
               trips_2016_Q2_05, trips_2016_Q2_06)
trips$starttime <- as_date(mdy_hm(trips$starttime))
trips$stoptime <- as_date(mdy_hm(trips$stoptime))
# starttime and stoptime for 2016 Q3 & Q4 are formatted differently
trips_2016_Q3Q4 <- rbind(trips_2016_Q3, trips_2016_Q4)
trips_2016_Q3Q4$starttime <- as_date(mdy_hms(trips_2016_Q3Q4$starttime))
trips_2016_Q3Q4$stoptime <- as_date(mdy_hms(trips_2016_Q3Q4$stoptime))
trips <- rbind(trips, trips_2016_Q3Q4)

names(trips)[names(trips) == "starttime"] <- "Date"
byday <- trips %>% 
  group_by(Date) %>% 
  summarise(Trips = n())


weather <- read_csv("chicagoweather.csv")
# convert date
weather$Date <- as_date(dmy(weather$Date))

# "T" stands for "trace", used when precipitation has been detected, but it isn't sufficient to measure meaningfully
weather$Precipitation[weather$Precipitation == "T"] <- 0
weather$Precipitation <- as.numeric(weather$Precipitation)

# treating events variable
weather[which(is.na(weather$Events)),]$Events <- "None"
weather$Events <- as.factor(weather$Events)
levels(weather$Events)
weather$Events <- gsub("Fog|Fog\\r\\t,\\rRain|Rain", "Rain", weather$Events)
weather$Events <- gsub("Fog\\r\\t,\\rRain\\r\\t,\\rSnow|Fog\\r\\t,\\rSnow|Rain\\r\\t,\\rSnow|Snow", "Snow", weather$Events)
weather$Events <- gsub("Fog\\r\\t,\\rRain\\r\\t,\\rThunderstorm|Rain\\r\\t,\\rHail\\r\\t,\\rThunderstorm|Rain\\r\\t,\\rThunderstorm|Thunderstorm", "Thunderstorm", weather$Events)
weather$Events <- as.factor(weather$Events)
levels(weather$Events)

# merge and create our training dataset!
train <- weather %>% 
  left_join(byday, by = "Date")

# write csv
write.csv(train, file = "train.csv", row.names = FALSE)
```

### Testing Data Set

We do the same transformation to trips and weather data for 2017 to create our testing set.

```{r test, eval = FALSE}
weather_2017 <- read_csv("chicagoweather2017.csv")
weather_2017$Date <- as_date(dmy(weather_2017$Date))
weather_2017[which(is.na(weather_2017$Precipitation)),]$Precipitation <- 0
weather_2017$Precipitation <- as.numeric(weather_2017$Precipitation)
weather_2017[which(is.na(weather_2017$Events)),]$Events <- "None"
weather_2017$Events <- as.factor(weather_2017$Events)
levels(weather_2017$Events)
weather_2017$Events <- gsub("Fog|Fog , Rain|Rain", "Rain", weather_2017$Events)
weather_2017$Events <- gsub("Fog , Rain , Snow|Fog , Snow|Rain , Snow|Snow", "Snow", weather_2017$Events)
weather_2017$Events <- gsub("Fog , Rain , Thunderstorm|Fog , Snow , Thunderstorm|Rain , Hail , Thunderstorm|Rain , Thunderstorm|Thunderstorm|Snow , Thunderstorm", "Thunderstorm", weather_2017$Events)
weather_2017$Events <- as.factor(weather_2017$Events)
levels(weather_2017$Events)

trips_2017_Q1 <- read_csv("Divvy_Trips_2017_Q1.csv")
trips_2017_Q2 <- read_csv("Divvy_Trips_2017_Q2.csv")
trips_2017 <- rbind(trips_2017_Q1, trips_2017_Q2)
trips_2017$Date <- as_date(mdy_hms(trips_2017$start_time))
byday2 <- trips_2017 %>% 
  group_by(Date) %>% 
  summarise(Trips = n())

test <- weather_2017 %>% 
  left_join(byday2, by = "Date")

write.csv(test, file = "test.csv", row.names = FALSE)
```
