---
title: "Divvy"
author: "Sungwan Kim"
date: "9/5/2017"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This is a prediction of bicycle sharing usage in the city of Chicago. I have defined the usage as the number of trips taken per day. I will use historical trips and weather datasets from 2014 to 2016 to predict the usage for the first half of 2017. I have decided to exclude trips data from 2013 as the number of rides during first few months are noticeably low as Divvy just started out its service in the Windy City.

## Load Data

```{r packages, results = 'hide'}
# Load packages
library('tidyverse')
library('lubridate')
library('leaps')
library('tree')
library('randomForest')
library('gbm')
```

To jump right into the Exploratory Data Analysis and Feature Engineering, I have written detailed data preparation at the end.

```{r, results = 'hide'}
train <- read_csv("train.csv")
test <- read_csv("test.csv")
```
```{r}
# check data
str(train)
```

There are few missing values in our training data set. These are from the days when there were no trips, so we input the adequate values accordingly.


```{r}
which(is.na(train$Trips))
train[which(is.na(train$Trips)),]$Trips <- 0
```


## Feature Engineering

Let's shed more light into what is driving the Divvy usage. First, I will define usage to be the number of trips taken per day.

```{r}
ggplot(train, aes(Date, Trips)) + geom_line()
```

The graph shows strong seasonal pattern. I will create a variable called quarter so that our models can capture this pattern when predicting.

```{r}
train$Quarter <- 1
train$Quarter[month(train$Date) %in% 4:6] <- 2
train$Quarter[month(train$Date) %in% 7:9] <- 3
train$Quarter[month(train$Date) %in% 10:12] <- 4
train$Quarter <- as.factor(train$Quarter)
```

I will create day of the week variable to see the shorter usage pattern.

```{r}
train <- train %>% 
  mutate(Wday = wday(Date, label = TRUE))
ggplot(train, aes(Wday, Trips)) + geom_boxplot()
```

It seems like usage during week days are little higher than usage during weekends.

```{r}
train %>% 
  ggplot(aes(Wday, Trips)) +
    geom_boxplot() +
    facet_grid(.~Quarter)
```

It seems like the usage for day of the week vary for different quarters.


## Prediction

```{r}
full <- rbind(train, test)
train <- full[1:1096,]
test <- full[1097:1277, ]
```

### Linear Regression

I will first use linear regression as a starting point

```{r}
lm.fit <- lm(Trips ~ Temperature_Avg + Dew_Point_Avg + Humidity_Avg + Sea_Level_Press_Avg + Visibility_Avg + Wind_Avg + Precipitation + Events + Wday + Quarter, train)
summary(lm.fit)
```

The p-value associated with the F-statistics suggest that we can reject the null hypothesis. This means at least one of the regressors are is associated with the increase in usage. The model has found temperature, sea level pressure, wind, precipitation, rain, thunderstorm, and quarter2 to be significant. R-squared is 0.6451 which means large portion of the variance is explained by our model.

So, how did our model do in terms of prediction? I have used Root Mean Squared Error(RMSE) which is a popular metric for assessing model accuracy. 

```{r}
# RMSE
(RMSE <- sqrt(mean((test$Trips - predict(lm.fit, test))^2)))
```

The linear model has a root mean squared error(RMSE) of `r RMSE`.

We might want to have an interaction term between day of the week and quarter variables as we have confirmed above that the usage differ by weekdays for different quarters.

```{r}
lm.fit2 <- lm(Trips ~ Temperature_Avg + Dew_Point_Avg + Humidity_Avg + Sea_Level_Press_Avg + Visibility_Avg + Wind_Avg + Precipitation + Events + Wday * Quarter, train)
summary(lm.fit2)
#RMSE
sqrt(mean((test$Trips - predict(lm.fit2, test))^2))
```

We did a little better in terms of the prediction.

One possible problem of the linear regression is non-constant variances in the errors, or heteroscedasticity. The residuals plot has a funnel shape which implies heteroskedasticity.

```{r}
par(mfrow=c(2,2))
plot(lm.fit2)
```

We can solve heteroskedasticity by log transformation. Since we have two trips value with 0, we cannot take log straight away. One way to bypass this problem is to add 1.

```{r}
lm.fit3 <- lm(log2(Trips+1) ~ Temperature_Avg + Dew_Point_Avg + Humidity_Avg + Sea_Level_Press_Avg + Visibility_Avg + Wind_Avg + Precipitation + Events + Wday * Quarter, train)
par(mfrow=c(2,2))
plot(lm.fit3)

# RMSE
sqrt(mean((test$Trips - 2^(predict(lm.fit3, test))-1)^2))
```

We treated heteroskedasticity, but the prediction result suffered as RMSE score skyrocketed. Another way to treat heteroskedasticy would be using square root.

```{r}
lm.fit4 <- lm(sqrt(Trips) ~ Temperature_Avg + Dew_Point_Avg + Humidity_Avg + Sea_Level_Press_Avg + Visibility_Avg + Wind_Avg + Precipitation + Events + Wday * Quarter, train)
par(mfrow=c(2,2))
plot(lm.fit4)

# RMSE
sqrt(mean((test$Trips - predict(lm.fit4, test)^2)^2))
```

We have done better than our orignial linear model! We have found a model which is both more accurate and robust.

Now, let's see if using all the features is the right way to run the regression. There might be issue of multicollinearity between weather variables as they are likely to be highly correlated.

```{r}
library(car)
vif(lm.fit)
```

To choose the optimal model with right numbers of varaibles we will consdier Adjusted R-squared, Bayesian information criterion(BIC), and Cp.

```{r}
regfit.full <- regsubsets(sqrt(Trips) ~ Temperature_Avg + Dew_Point_Avg + Humidity_Avg + Sea_Level_Press_Avg + Visibility_Avg + Wind_Avg + Precipitation + Events + Wday * Quarter, train)
reg.summary <- summary(regfit.full)
# R-squared statistic always increase with additional variable
reg.summary$rsq
# plotting RSS, adjusted R-squared, Cp, and BIC
par(mfrow=c(2,2))
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS", type="l")
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="l")
which.max(reg.summary$adjr2)
points(8, reg.summary$adjr2[8], col = "red", cex = 2, pch = 20)
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp", type="l")
which.min(reg.summary$cp)
points(8, reg.summary$cp[8], col = "red", cex = 2, pch = 20)
plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC", type="l")
which.min(reg.summary$bic)
points(8, reg.summary$bic[8], col = "red", cex = 2, pch = 20)
```

It is clear that using all 8 regressors is the right way to go.

### Decision Tree

Next, we consider the decision trees model to predict the usage.

```{r}
# set random seed
set.seed(11)
tree.divvy <- tree(Trips ~ Temperature_Avg + Dew_Point_Avg + Humidity_Avg + Sea_Level_Press_Avg + Visibility_Avg + Wind_Avg + Precipitation + Events + Wday + Quarter, train)
summary(tree.divvy)
par(mfrow=c(1, 1))
# plot tree
plot(tree.divvy)
text(tree.divvy, pretty = 0)
# RMSE
sqrt(mean((predict(tree.divvy, newdata = test) - test$Trips)^2))
```

We did better than linear models. We can do even better.

### Bagging, Random Forests, Boosting

Instead of using decision trees, we can use many trees to predict the usage. Bagging model is a special case of random forest when we use all the features available.

```{r}
set.seed(11)
bag.divvy <- randomForest(Trips ~ Temperature_Avg + Dew_Point_Avg + Humidity_Avg + Sea_Level_Press_Avg + Visibility_Avg + Wind_Avg + Precipitation + Events + Wday + Quarter, train, mtry=9, importance=TRUE)
sqrt(mean((predict(bag.divvy, newdata = test) - test$Trips)^2))
```


Now we use random forest. The default is using square root of features in this case 3.

```{r}
rf.divvy <- randomForest(Trips ~ Temperature_Avg + Dew_Point_Avg + Humidity_Avg + Sea_Level_Press_Avg + Visibility_Avg + Wind_Avg + Precipitation + Events + Wday + Quarter, train, mtry=3, importance=TRUE)

# RMSE
sqrt(mean((predict(rf.divvy, newdata = test) - test$Trips)^2))
```

Letâ€™s look at relative variable importance by plotting the mean decrease in Gini calculated across all trees.

```{r}
importance(rf.divvy)
varImpPlot(rf.divvy)
```

Temperature is the most important variable.


```{r}
set.seed(11)
boost.divvy <- gbm(Trips ~ Temperature_Avg + Dew_Point_Avg + Humidity_Avg + Sea_Level_Press_Avg + Visibility_Avg + Wind_Avg + Precipitation + Events + Wday + Quarter, train, distribution="gaussian", n.trees=5000, interaction.depth=4)
summary(boost.divvy)
#RMSE
sqrt(mean((predict(boost.divvy, newdata = test,n.trees=5000) - test$Trips)^2))
```



# Conclusion




## Data Preparation (Optional)

### Training Data Set

```{r datasets, results = 'hide'}
trips_2014_Q1Q2 <- read_csv("Divvy_Trips_2014_Q1Q2.csv")
trips_2014_Q3_07 <- read_csv("Divvy_Trips_2014-Q3-07.csv")
trips_2014_Q3_0809 <- read_csv("Divvy_Trips_2014-Q3-0809.csv")
trips_2014_Q4 <- read_csv("Divvy_Trips_2014-Q4.csv")
trips_2015_Q1 <- read_csv("Divvy_Trips_2015-Q1.csv")
trips_2015_Q2 <- read_csv("Divvy_Trips_2015-Q2.csv")
trips_2015_Q3_07 <- read_csv("Divvy_Trips_2015_07.csv")
trips_2015_Q3_08 <- read_csv("Divvy_Trips_2015_08.csv")
trips_2015_Q3_09 <- read_csv("Divvy_Trips_2015_09.csv")
trips_2015_Q4 <- read_csv("Divvy_Trips_2015_Q4.csv")
trips_2016_Q1 <- read_csv("Divvy_Trips_2016_Q1.csv")
trips_2016_Q2_04 <- read_csv("Divvy_Trips_2016_04.csv")
trips_2016_Q2_05 <- read_csv("Divvy_Trips_2016_05.csv")
trips_2016_Q2_06 <- read_csv("Divvy_Trips_2016_06.csv")
trips_2016_Q3 <- read_csv("Divvy_Trips_2016_Q3.csv")
trips_2016_Q4 <- read_csv("Divvy_Trips_2016_Q4.csv")
trips_2017_Q1 <- read_csv("Divvy_Trips_2017_Q1.csv")
trips_2017_Q2 <- read_csv("Divvy_Trips_2017_Q2.csv")

trips <- rbind(trips_2014_Q1Q2, trips_2014_Q3_07, trips_2014_Q3_0809, trips_2014_Q4, 
               trips_2015_Q1, trips_2015_Q2, trips_2015_Q3_07, trips_2015_Q3_08,
               trips_2015_Q3_09, trips_2015_Q4, trips_2016_Q1, trips_2016_Q2_04,
               trips_2016_Q2_05, trips_2016_Q2_06)
trips$starttime <- as_date(mdy_hm(trips$starttime))
trips$stoptime <- as_date(mdy_hm(trips$stoptime))
# starttime and stoptime for 2016 Q3 & Q4 are formatted differently
trips_2016_Q3Q4 <- rbind(trips_2016_Q3, trips_2016_Q4)
trips_2016_Q3Q4$starttime <- as_date(mdy_hms(trips_2016_Q3Q4$starttime))
trips_2016_Q3Q4$stoptime <- as_date(mdy_hms(trips_2016_Q3Q4$stoptime))
trips <- rbind(trips, trips_2016_Q3Q4)
```

Now, in order to get the usage value, we have to first group the number of trips taken by date. There are two options available for date to sum over - start and stop time. There are `r trips$starttime != trips$stoptime` instances where start time are different from the stop time. Since this is only samll percentage, we will use start time to sum together trips.

```{r}
names(trips)[names(trips) == "starttime"] <- "Date"
byday <- trips %>% 
  group_by(Date) %>% 
  summarise(Trips = n())
```

Now, let's take a look at weather data.

```{r, results = 'hide'}
weather <- read_csv("chicagoweather.csv")
# convert date
weather$Date <- as_date(dmy(weather$Date))

# "T" stands for "trace", used when precipitation has been detected, but it isn't sufficient to measure meaningfully
weather$Precipitation[weather$Precipitation == "T"] <- 0
weather$Precipitation <- as.numeric(weather$Precipitation)
```
```{r}
# treating events variable
weather[which(is.na(weather$Events)),]$Events <- "None"
weather$Events <- as.factor(weather$Events)
levels(weather$Events)
weather$Events <- gsub("Fog|Fog\\r\\t,\\rRain|Rain", "Rain", weather$Events)
weather$Events <- gsub("Fog\\r\\t,\\rRain\\r\\t,\\rSnow|Fog\\r\\t,\\rSnow|Rain\\r\\t,\\rSnow|Snow", "Snow", weather$Events)
weather$Events <- gsub("Fog\\r\\t,\\rRain\\r\\t,\\rThunderstorm|Rain\\r\\t,\\rHail\\r\\t,\\rThunderstorm|Rain\\r\\t,\\rThunderstorm|Thunderstorm", "Thunderstorm", weather$Events)
weather$Events <- as.factor(weather$Events)
levels(weather$Events)

# merge and create our training dataset!
train <- weather %>% 
  left_join(byday, by = "Date")

# write csv
write.csv(train, file = "train.csv", row.names = FALSE)
```

### Testing Data Set

We do the same transformation to trips and weather data for 2017 to create our testing set.

```{r, results = 'hide'}
weather_2017 <- read_csv("chicagoweather2017.csv")
weather_2017$Date <- as_date(dmy(weather_2017$Date))
weather_2017[which(is.na(weather_2017$Precipitation)),]$Precipitation <- 0
weather_2017$Precipitation <- as.numeric(weather_2017$Precipitation)
weather_2017[which(is.na(weather_2017$Events)),]$Events <- "None"
weather_2017$Events <- as.factor(weather_2017$Events)
levels(weather_2017$Events)
weather_2017$Events <- gsub("Fog|Fog , Rain|Rain", "Rain", weather_2017$Events)
weather_2017$Events <- gsub("Fog , Rain , Snow|Fog , Snow|Rain , Snow|Snow", "Snow", weather_2017$Events)
weather_2017$Events <- gsub("Fog , Rain , Thunderstorm|Fog , Snow , Thunderstorm|Rain , Hail , Thunderstorm|Rain , Thunderstorm|Thunderstorm|Snow , Thunderstorm", "Thunderstorm", weather_2017$Events)
weather_2017$Events <- as.factor(weather_2017$Events)
levels(weather_2017$Events)

trips_2017_Q1 <- read_csv("Divvy_Trips_2017_Q1.csv")
trips_2017_Q2 <- read_csv("Divvy_Trips_2017_Q2.csv")
trips_2017 <- rbind(trips_2017_Q1, trips_2017_Q2)

trips_2017$Date <- as_date(mdy_hms(trips_2017$start_time))
byday2 <- trips_2017 %>% 
  group_by(Date) %>% 
  summarise(Trips = n()) %>% 
  mutate(Wday = wday(Date, label = TRUE))

byday2$Quarter <- 1
byday2$Quarter[month(byday2$Date) %in% 4:6] <- 2
byday2$Quarter[month(byday2$Date) %in% 7:9] <- 3
byday2$Quarter[month(byday2$Date) %in% 10:12] <- 4
byday2$Quarter <- as.factor(byday2$Quarter)
levels(test$Quarter) <- levels(train$Quarter)

test <- weather_2017 %>% 
  left_join(byday2, by = "Date")

write.csv(test, file = "test.csv", row.names = FALSE)
```
